{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using spark-2.4.0-bin-hadoop2.7 along with jdk 8 and Kafka 2.12-2.1.1 .\n",
    "Note: I got issues with jdk 11 while using the RDD object, so, downgraded to jdk8.\n",
    "\n",
    "Pre-requisite - \n",
    "1) download http://central.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.0/spark-streaming-kafka-0-8-assembly_2.11-2.4.0.jar tp SPARK_HOME\\jars  directory. \n",
    "2) add this jar to path using following command before getting the stream object as given below in the code - \n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages C:\\\\myprograms\\\\spark-2.4.0-bin-hadoop2.7\\\\jars\\\\spark-streaming-kafka-0-8-assembly_2.11-2.4.0.jar pyspark-shell' #note that the \"pyspark-shell\" part is very important!!.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "#udf, concat, desc, window\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "# Use tqdm to show progress of an pandas function we use\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SparkSql SessionÂ¶\n",
    "local[*]  - use local node with as many worker threads as logical cores on your machine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(name=\"kafka_stream\").master(\"local[*]\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create streaming context with window=10secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc=StreamingContext(spark.sparkContext,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "zookeeper=\"localhost:2181\"#:9092\"\n",
    "groupid=\"sparkconsumer\"\n",
    "topic_name=\"topic_events\"\n",
    "partitions=3\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages C:\\\\myprograms\\\\spark-2.4.0-bin-hadoop2.7\\\\jars\\\\spark-streaming-kafka-0-8-assembly_2.11-2.4.0.jar pyspark-shell' #note that the \"pyspark-shell\" part is very important!!.\n",
    "\n",
    "stream = KafkaUtils.createStream(ssc, zookeeper, groupid, {topic_name:partitions})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the events from the Dstream object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stream has data like this - (None, 'd1 e1 f1')\n",
    "#stream.pprint()\n",
    "\n",
    "dstream = stream.map(lambda x: x[1])\n",
    "#lines contains the content(values) from kafka stream (k,v) pair.\n",
    "\n",
    "#lines.pprint()\n",
    "#print(type(lines))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group the events that come within 20 seconds window, with sliding window=10 secs.\n",
    "Result dstream of this function will contain a single RDD.\n",
    "This RDD will contain all event messages we received within that sliding window of 20 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstream=dstream.reduceByWindow(lambda x,y: \"{} {}\".format(x,y),\n",
    "                     None,20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a sample function.\n",
    "#In-parameter contains the list of events we received during the sliding window of 20 seconds.\n",
    "#This one just prints the list. However, ideally, we can send this list over to a processing thread.\n",
    "def process_event_groups(lst):\n",
    "    print(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass on the event list within the window to a processing function.\n",
    "The reduced dstream contains single RDD.\n",
    "<br/> Pass on the contents of this RDD to the processing function.\n",
    "<br/> ie., The list of events within the 20 mins sliding window will be passed on to the processing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstream.foreachRDD(lambda x: process_event_groups(x.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start receiving from the stream. This will execute all the functions on the 'stream' object that we defined above, for each of the '10' seconds window.\n",
    "As we are using the reduceByWindow, the events will be grouped as per this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "['a1 b1 c1 d1 e1']\n",
      "['a1 b1 c1 d1 e1 f1 g1']\n",
      "['f1 g1 h1 j1']\n",
      "['h1 j1 k1 l1 m1']\n",
      "['k1 l1 m1']\n",
      "['n1 o1 p1']\n",
      "['n1 o1 p1']\n"
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
